{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3Cc6DBn24UmKdW9s4PVgUv9",
      "metadata": {
        "id": "c3Cc6DBn24UmKdW9s4PVgUv9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "class MissingTokenError(ValueError):\n",
        "    \"\"\"Custom exception for missing GitHub token.\"\"\"\n",
        "    pass\n",
        "\n",
        "def fetch_repos_issues_by_name(repo_name: str, owner: str, issue_count=5, token=None):\n",
        "    \"\"\"\n",
        "    Fetch issues from a GitHub repository, sorted by creation date in descending order.\n",
        "\n",
        "    Args:\n",
        "        repo_name (str): The name of the repository.\n",
        "        owner (str): The owner of the repository.\n",
        "        issue_count (int): Number of issues to fetch. Default is 10.\n",
        "        token (str): GitHub personal access token.\n",
        "\n",
        "    Returns:\n",
        "        dict: JSON response containing the issues.\n",
        "\n",
        "    Raises:\n",
        "        MissingTokenError: If the token is not provided.\n",
        "        Exception: If the request fails for any other reason.\n",
        "    \"\"\"\n",
        "    if token is None:\n",
        "        raise MissingTokenError(\"GitHub token is required to authenticate the request.\")\n",
        "\n",
        "    query = f\"\"\"\n",
        "    query {{\n",
        "      repository(owner: \"{owner}\", name: \"{repo_name}\") {{\n",
        "        issues(first: {issue_count}, orderBy: {{ field: CREATED_AT, direction: DESC }}) {{\n",
        "          edges {{\n",
        "            node {{\n",
        "              title\n",
        "              number\n",
        "              createdAt\n",
        "              url\n",
        "              bodyText\n",
        "              comments(first: 10) {{\n",
        "                edges {{\n",
        "                  node {{\n",
        "                    author {{\n",
        "                      login\n",
        "                    }}\n",
        "                    bodyText\n",
        "                    createdAt\n",
        "                  }}\n",
        "                }}\n",
        "              }}\n",
        "            }}\n",
        "          }}\n",
        "        }}\n",
        "      }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(\n",
        "        \"https://api.github.com/graphql\",\n",
        "        json={\"query\": query},\n",
        "        headers=headers\n",
        "    )\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        raise Exception(f\"Request failed with status code {response.status_code}: {response.json()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ZHnNtUTqmymZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHnNtUTqmymZ",
        "outputId": "9ee6a791-d722-4415-c8c8-8ed77bca3bef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': {'repository': {'issues': {'edges': [{'node': {'title': \"MNT Make binary display method parameters' order consistent\", 'number': 30717, 'createdAt': '2025-01-25T11:39:26Z', 'url': 'https://github.com/scikit-learn/scikit-learn/issues/30717', 'bodyText': 'This came up while working on #30399 . These are all classes inheriting the _BinaryClassifierCurveDisplayMixin.\\n\\nRocCurveDisplay and PrecisionRecallDisplay are pretty consistent, we would just need to change where pos_label is. No strong preference to where it should be.\\nDetCurveDisplay does not have the chance level line, drop_intermediate and depsine. Chance line is added in #29151 (we should ensure order is consistent in that PR). Note there is discussion of adding drop_intermediate in that PR as well\\nCalibrationDisplay - is a bit different from the rest, e.g., there is a reference line (perfect calibration) and not a chance line. We could move ax up though, to be consistent with the other displays.\\n\\n\\nTable of parameters\\n\\n\\n\\n\\nCalibrationDisplay\\nDetCurveDisplay\\nRocCurveDisplay\\nPrecisionRecallDisplay\\n\\n\\n\\n\\nplot\\naxnameref_linekwargs\\naxnamekwargs\\naxnameplot_chance_levelchance_level_kwdespinekwargs\\naxnameplot_chance_levelchance_level_kwdespinekwargs\\n\\n\\nfrom_estimator\\nestimatorXyn_binsstrategypos_labelnameref_lineaxkwargs\\nestimatorXysample_weightresponse_methodpos_labelnameaxkwargs\\nestimatorXysample_weightdrop_intermediateresponse_methodpos_labelnameaxplot_chance_levelchance_level_kwdespinekwargs\\nestimatorXysample_weightpos_labeldrop_intermediateresponse_methodnameaxplot_chance_levelchance_level_kwdespinekwargs\\n\\n\\nfrom_predictions\\ny_truey_probn_binsstrategypos_labelnameref_lineaxkwargs\\ny_truey_predsample_weightpos_labelnameaxkwargs\\ny_truey_predsample_weightdrop_intermediatepos_labelnameaxplot_chance_levelchance_level_kwdespinekwargs\\ny_truey_predsample_weightpos_labeldrop_intermediatenameaxplot_chance_levelchance_level_kwdespinekwargs\\n\\n\\n\\n\\nDiscussed with @DeaMariaLeon @glemaitre', 'comments': {'edges': [{'node': {'author': {'login': 'DeaMariaLeon'}, 'bodyText': '/take', 'createdAt': '2025-01-25T14:10:08Z'}}]}}}, {'node': {'title': 'Version 1.0.2 requires numpy<2', 'number': 30714, 'createdAt': '2025-01-24T11:47:50Z', 'url': 'https://github.com/scikit-learn/scikit-learn/issues/30714', 'bodyText': 'Describe the bug\\nInstalling scikit-learn version 1.0.2 leads to the following error:\\nValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\\nThis seems to indicate a mismatch between this version of scikit-learn and numpy versions greater than 2.0 (Specifically 2.2.2 was being installed, following the only restriction of numpy>=1.14.6).\\nThis can be solved by indicating to use a numpy version older than 2.0 by modifying step 1 to:\\npip install \"scikit-learn==1.0.2\" \"numpy<2\"\\nAdditional references\\nhttps://stackoverflow.com/questions/66060487/valueerror-numpy-ndarray-size-changed-may-indicate-binary-incompatibility-exp\\nhttps://stackoverflow.com/questions/78650222/valueerror-numpy-dtype-size-changed-may-indicate-binary-incompatibility-expec\\nSteps/Code to Reproduce\\n\\nInstall scikit-learn through pip\\n\\npip install \"scikit-learn==1.0.2\"\\n\\nUse scikit-learn\\n\\n% path/to/script.py\\n...\\nfrom sklearn.datasets import load_iris\\n...\\nExpected Results\\nNo errors thrown\\nActual Results\\nError is thrown:\\npath/to/script.py:2: in <module>\\n    from sklearn.datasets import load_iris\\n/opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/sklearn/__init__.py:82: in <module>\\n    from .base import clone\\n/opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/sklearn/base.py:17: in <module>\\n    from .utils import _IS_32BIT\\n/opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/sklearn/utils/__init__.py: in <module>\\n    from .murmurhash import murmurhash3_32\\nsklearn/utils/murmurhash.pyx:1: in init sklearn.utils.murmurhash\\n    ???\\nE   ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\\nVersions\\nOS: Ubuntu 24.10 (latest)\\nPython version 3.10\\nScikit-learn version: 1.0.2\\npip version: 24.3.1\\nsetuptools version: 65.5.0', 'comments': {'edges': [{'node': {'author': {'login': 'lesteve'}, 'bodyText': 'Thanks for the issue, indeed it is less than great for users, but doing something about it is not trivial at all, see #29630 (comment) for more details why.\\nSo for now, the consensus is that people will hopefully find issues like yours with enough details to fix the problem themselves.\\nOut of curiosity, do you have a reason to install scikit-learn=1.0.2 which has been released a while ago (December 2021)? One known reason is that you are using python 3.7.', 'createdAt': '2025-01-24T12:48:07Z'}}, {'node': {'author': {'login': 'lesteve'}, 'bodyText': 'I am going to close this issue because it is unlikely that we will do anything about it, thanks for your understanding üôè!', 'createdAt': '2025-01-24T12:48:43Z'}}, {'node': {'author': {'login': 'ogrisel'}, 'bodyText': 'I would also be genuinely interested to know the reasons to install such an old version of scikit-learn. Are there too many breaking changes in newer versions that would require significant changes in your code or in some dependencies of your deployment?', 'createdAt': '2025-01-24T13:16:52Z'}}, {'node': {'author': {'login': 'grudloff'}, 'bodyText': '@lesteve Totally understandable, I just wanted to report it here, as it took me some time to figure it out.\\n@ogrisel GCP prebuilt training containers for scikit learn have this as the most recent version (see here), so if you want to test locally before training on the cloud you need to use the same version.', 'createdAt': '2025-01-24T14:02:16Z'}}, {'node': {'author': {'login': 'ogrisel'}, 'bodyText': 'This is very interesting feedback! That might explain (part of) a mystery about our pypi download statistics.\\nGiven this information, maybe we might want to make the effort to issue .post0 release to upper bound the version of numpy for older scikit-learn versions.', 'createdAt': '2025-01-24T14:12:38Z'}}, {'node': {'author': {'login': 'lesteve'}, 'bodyText': \"@grudloff thanks for the feed-back indeed and detailed issue, that may help more people fixing their own issue üôè.\\n@ogrisel I don't think this explains completely why 1.0.2 is our most popular release because last time I looked 1.0.2 downloads were mostly coming from Python 3.7 see plot below (and GCP prebuilt container with 1.0.2 is Python 3.10 if I understood the link mentioned above), but maybe that's still something to think about.\", 'createdAt': '2025-01-24T14:39:35Z'}}, {'node': {'author': {'login': 'grudloff'}, 'bodyText': 'Glad to help!', 'createdAt': '2025-01-24T14:59:59Z'}}]}}}, {'node': {'title': 'Error in `d2_log_loss_score` multiclass when one of the classes is missing in `y_true`.', 'number': 30713, 'createdAt': '2025-01-24T11:01:39Z', 'url': 'https://github.com/scikit-learn/scikit-learn/issues/30713', 'bodyText': 'Describe the bug\\nHello, I encountered an error with the d2_log_loss_score in the multiclass setting (i.e. when y_pred has shape (n, k) with k >= 3) when one of the classes is missing from the y_true labels, even when giving the labels through the labels argument. The error disappear when all the classes are present in y_true.\\nSteps/Code to Reproduce\\nfrom sklearn.metrics import d2_log_loss_score\\n\\ny_true = [0, 1, 1]\\ny_pred = [[1, 0, 0], [1, 0, 0], [1, 0, 0]]\\nlabels = [0, 1, 2]\\n\\nd2_log_loss_score(y_true, y_pred, labels=labels)\\nExpected Results\\nNo error is thrown.\\nActual Results\\nTraceback (most recent call last):\\n  File \"minimal.py\", line 7, in <module>\\n    d2_log_loss_score(y_true, y_pred, labels=labels)\\n  File \".../python3.12/site-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \".../python3.12/site-packages/sklearn/metrics/_classification.py\", line 3407, in d2_log_loss_score\\n    denominator = log_loss(\\n                  ^^^^^^^^^\\n  File \".../python3.12/site-packages/sklearn/utils/_param_validation.py\", line 189, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \".../python3.12/site-packages/sklearn/metrics/_classification.py\", line 3023, in log_loss\\n    raise ValueError(\\nValueError: The number of classes in labels is different from that in y_pred. Classes found in labels: [0 1 2]\\n\\nVersions\\nSystem:\\n    python: 3.12.8 | packaged by conda-forge | (main, Dec  5 2024, 14:19:53) [Clang 18.1.8 ]\\nexecutable: /Users/alexandreperez/dev/lib/miniforge3/envs/test/bin/python\\n   machine: macOS-15.2-arm64-arm-64bit\\n\\nPython dependencies:\\n      sklearn: 1.6.1\\n          pip: 24.3.1\\n   setuptools: 75.8.0\\n        numpy: 2.2.2\\n        scipy: 1.15.1\\n       Cython: None\\n       pandas: None\\n   matplotlib: None\\n       joblib: 1.4.2\\nthreadpoolctl: 3.5.0\\n\\nBuilt with OpenMP: True\\n\\nthreadpoolctl info:\\n       user_api: openmp\\n   internal_api: openmp\\n    num_threads: 14\\n         prefix: libomp\\n       filepath: .../miniforge3/envs/test/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib\\n        version: None', 'comments': {'edges': [{'node': {'author': {'login': 'lesteve'}, 'bodyText': 'Indeed, I can reproduce, looks like an oversight, but more investigation is needed.', 'createdAt': '2025-01-24T14:22:13Z'}}]}}}, {'node': {'title': 'Add sample_weight support to QuantileTransformer', 'number': 30707, 'createdAt': '2025-01-22T23:07:51Z', 'url': 'https://github.com/scikit-learn/scikit-learn/issues/30707', 'bodyText': 'Describe the workflow you want to enable\\nWould be good to get sample_weight support for QuantileTransformer for dealing with sparse or imbalanced data, a la #15601.\\nscaler = QuantileTransformer(output_distribution=\"normal\")\\n\\nscaler.fit(X, sample_weight=w)\\n\\n\\nDescribe your proposed solution\\nAs far as I know it would just require adding the weight argument to the quantiles_ computation in np.nanpercentile.\\nKBinsDiscretizer supports sample_weight and with strategy=\\'quantile\\', encode=\\'ordinal\\' this behavior can be achieved but it is much, much slower.\\nDescribe alternatives you\\'ve considered, if relevant\\nNo response\\nAdditional context\\nNo response', 'comments': {'edges': [{'node': {'author': {'login': 'lesteve'}, 'bodyText': 'Searching in the issue tracker, looks like this feature is in scope if I understand correctly #20522 (comment).\\ncc @snath-xoc and @jeremiedbb for an informed opinion, since they have been working on sample weights support recently.', 'createdAt': '2025-01-23T12:56:13Z'}}]}}}, {'node': {'title': 'CI Use explicit permissions for GHA workflows', 'number': 30702, 'createdAt': '2025-01-22T15:34:17Z', 'url': 'https://github.com/scikit-learn/scikit-learn/issues/30702', 'bodyText': 'CodeQL scanning is nudging us towards using explicit permission, see https://github.com/scikit-learn/scikit-learn/security/code-scanning?query=is%3Aopen+branch%3Amain+rule%3Aactions%2Fmissing-workflow-permissions\\nThis is an excuse to try out sub-issues üòÖ Apparently you can not add PR as sub-issue oh well üòì ?', 'comments': {'edges': []}}}, {'node': {'title': 'Make scikit-learn OpenML more generic for the data download URL', 'number': 30699, 'createdAt': '2025-01-22T09:13:44Z', 'url': 'https://github.com/scikit-learn/scikit-learn/issues/30699', 'bodyText': 'According to https://github.com/orgs/openml/discussions/20#discussioncomment-11913122 our code hardcodes where to find the OpenML data.\\nI am not quite sure what needs to be done right now but maybe @PGijsbers has some suggestions (not urgent at all though, I am guessing you have bigger fish to fry right now üòâ) or maybe @glemaitre .', 'comments': {'edges': [{'node': {'author': {'login': 'PGijsbers'}, 'bodyText': \"The url field in the dataset description should always point to a location where the arff file can be found, and the parquet_url field should always point to the parquet file. If that is not the case, that's an error with OpenML. If there are particular issues preventing you from using the links as provided, we'd be happy to help look for a solution.\", 'createdAt': '2025-01-22T10:44:36Z'}}, {'node': {'author': {'login': 'joaquinvanschoren'}, 'bodyText': 'The production server is back. fetch_openml seems to work again. üéâ\\nPlease let us know if you run into any other issues.', 'createdAt': '2025-01-24T19:16:17Z'}}, {'node': {'author': {'login': 'joaquinvanschoren'}, 'bodyText': \"The parquet_url links won't work at the moment. We need to update these links server-side.\", 'createdAt': '2025-01-24T19:17:59Z'}}]}}}, {'node': {'title': 'Inaccurate error message for parameter passing in Pipeline with enable_metadata_routing=True', 'number': 30692, 'createdAt': '2025-01-21T19:08:21Z', 'url': 'https://github.com/scikit-learn/scikit-learn/issues/30692', 'bodyText': 'Describe the issue linked to the documentation\\nThe following error message is inaccurate:\\nPassing extra keyword arguments to Pipeline.transform is only supported if enable_metadata_routing=True, which you can set using sklearn.set_config.  \\n\\nThis can easily be done using **params as described in the documentation for sklearn.pipeline: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.fit\\nPlease consider the following example:\\nfrom sklearn.base import BaseEstimator, TransformerMixin\\nfrom sklearn.pipeline import Pipeline\\nfrom scipy.sparse import csr_matrix\\nimport pandas as pd\\nimport numpy as np\\n\\nclass DummyTransformer(BaseEstimator, TransformerMixin):\\n    def __init__(self):\\n        self.feature_index_sec = None  # initialize attribute\\n\\n    def transform(self, X, feature_index_sec=None, **fit_params):\\n        if feature_index_sec is None:\\n            raise ValueError(\"Missing required argument \\'feature_index_sec\\'.\")\\n            \\n        print(f\"Transform Received feature_index_sec with shape: {feature_index_sec.shape}\")\\n        return X\\n\\n    def fit(self, X, y=None, feature_index_sec=None, **fit_params):\\n        print(f\"Fit Received feature_index_sec with shape: {feature_index_sec.shape}\")\\n        return self\\n\\n    def fit_transform(self, X, y=None, feature_index_sec=None, **fit_params):\\n        self.fit(X, y, feature_index_sec, **fit_params)  # feature_index_sec is passed with other parameters\\n        return self.transform(X, feature_index_sec, **fit_params)\\n\\nfeature_matrix = csr_matrix(np.random.rand(10, 5))\\ntrain_idx = pd.DataFrame({\\'FileDate_ClosingPrice\\': np.random.rand(10)})\\n\\ntransformer = DummyTransformer()\\npipe = Pipeline(steps=[(\\'DummyTransformer\\', transformer)])\\n\\npipe.fit_transform(feature_matrix, DummyTransformer__feature_index_sec=train_idx)\\n\\n# this line creates the error\\npipe.transform(feature_matrix, DummyTransformer__feature_index_sec=train_idx)\\nWhich outputs:\\nFit Received feature_index_sec with shape: (10, 1)\\nTransform Received feature_index_sec with shape: (10, 1)\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\nFile /tmp/test.py:35\\n     32 pipe.fit_transform(feature_matrix, DummyTransformer__feature_index_sec=train_idx)\\n     34 # this line creates the error\\n---> 35 pipe.transform(feature_matrix, DummyTransformer__feature_index_sec=train_idx)\\n\\nFile ~/micromamba/lib/python3.12/site-packages/sklearn/pipeline.py:896, in Pipeline.transform(self, X, **params)\\n    863 @available_if(_can_transform)\\n    864 def transform(self, X, **params):\\n    865     \"\"\"Transform the data, and apply `transform` with the final estimator.\\n    866 \\n    867     Call `transform` of each transformer in the pipeline. The transformed\\n   (...)\\n    894         Transformed data.\\n    895     \"\"\"\\n--> 896     _raise_for_params(params, self, \"transform\")\\n    898     # not branching here since params is only available if\\n    899     # enable_metadata_routing=True\\n    900     routed_params = process_routing(self, \"transform\", **params)\\n\\nFile ~/micromamba/lib/python3.12/site-packages/sklearn/utils/_metadata_requests.py:158, in _raise_for_params(params, owner, method)\\n    154 caller = (\\n    155     f\"{owner.__class__.__name__}.{method}\" if method else owner.__class__.__name__\\n    156 )\\n    157 if not _routing_enabled() and params:\\n--> 158     raise ValueError(\\n    159         f\"Passing extra keyword arguments to {caller} is only supported if\"\\n    160         \" enable_metadata_routing=True, which you can set using\"\\n    161         \" `sklearn.set_config`. See the User Guide\"\\n    162         \" <https://scikit-learn.org/stable/metadata_routing.html> for more\"\\n    163         f\" details. Extra parameters passed are: {set(params)}\"\\n    164     )\\n\\nValueError: Passing extra keyword arguments to Pipeline.transform is only supported if enable_metadata_routing=True, which you can set using `sklearn.set_config`. See the User Guide <https://scikit-learn.org/stable/metadata_routing.html> for more details. Extra parameters passed are: {\\'DummyTransformer__feature_index_sec\\'}\\n\\nRequest\\n\\nThe error message should be updated to clarify that parameters can already be passed using the **params (e.g., StepName__param_name) syntax, which is unrelated to metadata routing.\\nAdditionally, I was unable to find any example of using enable_metadata_routing=True to pass parameters, either in the documentation or in the wild. It would be helpful if the documentation provided a working example of passing parameters using metadata routing, especially for custom transformers.\\n\\nSuggest a potential alternative/fix\\nNo response', 'comments': {'edges': [{'node': {'author': {'login': 'lesteve'}, 'bodyText': 'Thanks for the detailed issue!\\nTo help us debug the issue, could you post a snippet that actually reproduces the error Passing extra keyword arguments to Pipeline.transform is only supported if enable_metadata_routing=True, which you can set using sklearn.set_config.?', 'createdAt': '2025-01-22T09:32:22Z'}}, {'node': {'author': {'login': 'lesteve'}, 'bodyText': 'Ah I think I figured it out, you need to call .transform I have edited your snippet to do it.', 'createdAt': '2025-01-22T09:36:30Z'}}, {'node': {'author': {'login': 'lesteve'}, 'bodyText': 'We looked a bit at it with @StefanieSenger and my current understanding is the following:\\n\\nthe error message is kind of right (improvement suggestions would be more than welcome of course).\\nif you want to pass arguments to Pipeline.transform maybe you want to give a go to metadata routing (not a metada-routing expert so I could well be wrong)\\n\\ncc @adrinjalali who may have a more informed opinion.', 'createdAt': '2025-01-22T09:52:31Z'}}, {'node': {'author': {'login': 'jakemdrew'}, 'bodyText': 'The challenge is that I cannot even find a working example of using\\nmetadata routing to pass feature_index_sec via .transform().\\nAdding set_config(enable_metadata_routing=True) does not help, and produces\\nthe error \"TypeError: Pipeline.transform got unexpected argument(s)\\n{\\'DummyTransformer__feature_index_sec\\'}, which are not routed to any\\nobject.\" ChatGPT cannot solve this question.  I have tried many variations\\nand also posted it on stackoverflow here:\\n\\nhttps://stackoverflow.com/questions/79372602/pass-parameters-to-custom-transformer-in-sklearn/79373117#79373117\\n\\nThe accepted answer does not work for .transform().\\n‚Ä¶\\nOn Wed, Jan 22, 2025 at 3:52\\u202fAM Lo√Øc Est√®ve ***@***.***> wrote:\\n We looked a bit at it with @StefanieSenger\\n <https://github.com/StefanieSenger> and my current understanding is the\\n following:\\n\\n    - the error message is kind of right (improvement suggestions would be\\n    more than welcome of course).\\n    - if you want to pass arguments to Pipeline.transform maybe you want\\n    to give a go to metadata routing (not a metada-routing expert so I could\\n    well be wrong)\\n\\n cc @adrinjalali <https://github.com/adrinjalali> who may have a more\\n informed opinion.\\n\\n ‚Äî\\n Reply to this email directly, view it on GitHub\\n <#30692 (comment)>,\\n or unsubscribe\\n <https://github.com/notifications/unsubscribe-auth/AAQCS46NAMF3WZ5IGQRN5JT2L5S7LAVCNFSM6AAAAABVTHJOU2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMMBWG43TAOBVGY>\\n .\\n You are receiving this because you authored the thread.Message ID:\\n ***@***.***>', 'createdAt': '2025-01-22T23:40:13Z'}}, {'node': {'author': {'login': 'StefanieSenger'}, 'bodyText': 'Hi @jakemdrew,\\ncan you please provide a reproducible of the error, so we can see what you had expected to work or where you thought the error message was incorrect? Thanks a lot for your efforts.\\nEdit:\\nI now see. I will have a further look into this in the coming days.', 'createdAt': '2025-01-23T05:53:06Z'}}, {'node': {'author': {'login': 'jakemdrew'}, 'bodyText': 'Things that do not make sense:\\n\\n**.transform() will receive params via .fit_transform().  However, it will throw this error when called directly: \"ValueError: Passing extra keyword arguments to Pipeline.transform is only supported if enable_metadata_routing=True, which you can set using sklearn.set_config. See the User Guide https://scikit-learn.org/stable/metadata_routing.html for more details. Extra parameters passed are: {\\'DummyTransformer__feature_index_sec\\'}\"\\nFollowing the error message above, if you add enable_metadata_routing=True (uncomment the lines below), you will get \"TypeError: Pipeline.fit_transform got unexpected argument(s) {\\'DummyTransformer__feature_index_sec\\'}, which are not routed to any object.\"\\nThe documentation for Metadata Routing is anything but clear.  For example, ChatGPT cannot draft a proper answer for how to pass feature_index_sec to DummyTransformer.transform() below even when you provide it the Metadata Routing documentation.\\n\\nCode to Reproduce Error Messages\\nfrom sklearn.base import BaseEstimator, TransformerMixin\\nfrom sklearn.pipeline import Pipeline\\nfrom scipy.sparse import csr_matrix\\nimport pandas as pd\\nimport numpy as np\\n\\n#from sklearn import set_config\\n#set_config(enable_metadata_routing=True)\\n\\nclass DummyTransformer(BaseEstimator, TransformerMixin):\\n    def __init__(self):\\n        self.feature_index_sec = None  # initialize attribute\\n\\n    def transform(self, X, feature_index_sec=None, **fit_params):\\n        if feature_index_sec is None:\\n            raise ValueError(\"Missing required argument \\'feature_index_sec\\'.\")\\n            \\n        print(f\"Transform Received feature_index_sec with shape: {feature_index_sec.shape}\")\\n        return X\\n\\n    def fit(self, X, y=None, feature_index_sec=None, **fit_params):\\n        print(f\"Fit Received feature_index_sec with shape: {feature_index_sec.shape}\")\\n        return self\\n\\n    def fit_transform(self, X, y=None, feature_index_sec=None, **fit_params):\\n        self.fit(X, y, feature_index_sec, **fit_params)  # feature_index_sec is passed with other parameters\\n        return self.transform(X, feature_index_sec, **fit_params)\\n\\nfeature_matrix = csr_matrix(np.random.rand(10, 5))\\ntrain_idx = pd.DataFrame({\\'FileDate_ClosingPrice\\': np.random.rand(10)})\\n\\ntransformer = DummyTransformer()\\npipe = Pipeline(steps=[(\\'DummyTransformer\\', transformer)])\\n\\n# This passes **params to transform() \\npipe.fit_transform(feature_matrix, DummyTransformer__feature_index_sec=train_idx)\\n\\n# This fails validation and produces the error\\npipe.transform(feature_matrix, DummyTransformer__feature_index_sec=train_idx)\\n\\nSimple Example: How to Implement Metadata Routing\\nBaseEstimator Now Inherits from _MetadataRequester:\\nScikit-learn\\'s BaseEstimator inherits from _MetadataRequester, which provides a suite of methods for defining metadata routing:\\n\\nset_fit_request\\nset_transform_request\\nOther similar methods like set_predict_request, set_score_request, etc.\\n\\nThree Steps for Using Metadata Routing:\\n\\nEnable metadata routing globally with set_config(enable_metadata_routing=True).\\nCreate an instance of the estimator or transformer and call the appropriate set_*_request methods to define which metadata is expected.\\nPass **params  and the proper metadata variable (like feature_index_sec) to the fit, transform, or other relevant methods.\\n\\nWorking Example with DummyTransformer:\\nThe DummyTransformer below prints out the metadata received in its fit and transform methods.\\nSimple Metadata Routing Code Example\\nfrom sklearn.base import BaseEstimator, TransformerMixin\\nfrom sklearn.pipeline import Pipeline\\nfrom scipy.sparse import csr_matrix\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom sklearn import set_config\\nset_config(enable_metadata_routing=True)\\n\\nclass DummyTransformer(BaseEstimator, TransformerMixin):\\n    def __init__(self):\\n        self.feature_index_sec = None  # initialize attribute\\n\\n    def transform(self, X, feature_index_sec=None, **fit_params):\\n        if feature_index_sec is None:\\n            raise ValueError(\"Missing required argument \\'feature_index_sec\\'.\")\\n            \\n        print(f\"Transform Received feature_index_sec with shape: {feature_index_sec.shape}\")\\n        return X\\n\\n    def fit(self, X, y=None, feature_index_sec=None, **fit_params):\\n        print(f\"Fit Received feature_index_sec with shape: {feature_index_sec.shape}\")\\n        return self\\n\\n    def fit_transform(self, X, y=None, feature_index_sec=None, **fit_params):\\n        self.fit(X, y, feature_index_sec, **fit_params)  # feature_index_sec is passed with other parameters\\n        return self.transform(X, feature_index_sec, **fit_params)\\n\\nfeature_matrix = csr_matrix(np.random.rand(10, 5))\\ntrain_idx = pd.DataFrame({\\'FileDate_ClosingPrice\\': np.random.rand(10)})\\n\\ntransformer = (\\n    DummyTransformer()\\n    .set_fit_request(feature_index_sec=True)\\n    .set_transform_request(feature_index_sec=True)\\n)\\n\\npipe = Pipeline(steps=[(\\'DummyTransformer\\', transformer)])\\n\\n# Test routing metadata via .fit_transform() and .transform() methods\\npipe.fit_transform(feature_matrix, feature_index_sec=train_idx)\\npipe.transform(feature_matrix, feature_index_sec=train_idx)\\n\\nOutputs\\nFit Received feature_index_sec with shape: (10, 1)\\nTransform Received feature_index_sec with shape: (10, 1)\\nTransform Received feature_index_sec with shape: (10, 1)\\n<10x5 sparse matrix of type \\'<class \\'numpy.float64\\'>\\'\\nwith 50 stored elements in Compressed Sparse Row format>', 'createdAt': '2025-01-23T17:55:04Z'}}]}}}, {'node': {'title': 'FeatureHasher and HashingVectorizer does not expose requires_fit=False tag', 'number': 30689, 'createdAt': '2025-01-21T10:28:21Z', 'url': 'https://github.com/scikit-learn/scikit-learn/issues/30689', 'bodyText': 'While FeatureHasher and HashingVectorizer are stateless estimator (at least in their docstrings), they do not expose the requires_fit tag to False as other stateless estimator.\\n@adrinjalali Do you recall when changing the tags if there was a particular reason for those estimator to not behave the same way than others?', 'comments': {'edges': [{'node': {'author': {'login': 'adrinjalali'}, 'bodyText': \"My PR didn't change a tag in that regard:\\nhttps://github.com/scikit-learn/scikit-learn/pull/29677/files#diff-2ce614636a32fc7ddace8d5f36e320e0b348012dff967ad17c7fed8ea40b5723R196\\nNote that requires_fit is tricky if the estimator does input / feature name / feature count validation.\", 'createdAt': '2025-01-21T11:36:25Z'}}]}}}, {'node': {'title': '‚ö†Ô∏è CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Jan 21, 2025) ‚ö†Ô∏è', 'number': 30684, 'createdAt': '2025-01-21T02:49:32Z', 'url': 'https://github.com/scikit-learn/scikit-learn/issues/30684', 'bodyText': 'CI failed on Linux_Runs.pylatest_conda_forge_mkl (Jan 21, 2025)\\n\\ntest_linear_regression_sample_weights[95-True-csr_matrix]\\ntest_linear_regression_sample_weights[95-True-csr_array]', 'comments': {'edges': [{'node': {'author': {'login': 'lesteve'}, 'bodyText': \"Seems very similar to #29909 (comment), and indeed I can reproduce locally ...\\ngit bisect points at #30521, maybe @SuccessMoses, @ogrisel or @OmarManzoor have some insights into this? I guess it may be a matter of setting a smaller tol in the test or something similar?\\nTo reproduce the test failure on my machine:\\nSKLEARN_TESTS_GLOBAL_RANDOM_SEED=all pytest sklearn/linear_model/tests/test_base.py \\\\\\n    -k 'test_linear_regression_sample_weights and 95' -v -s\\nThe difference seems quite big ~3e-3 compared to rtol=1e-7:\\nE       AssertionError: \\nE       Not equal to tolerance rtol=1e-07, atol=0\\nE       \\nE       Mismatched elements: 5 / 5 (100%)\\nE       Max absolute difference among violations: 0.00075961\\nE       Max relative difference among violations: 0.00295252\\nE        ACTUAL: array([ 0.06997 ,  0.258036, -0.61785 ,  0.439414, -0.790052])\\nE        DESIRED: array([ 0.069826,  0.257276, -0.617861,  0.439693, -0.790149])\", 'createdAt': '2025-01-21T09:58:03Z'}}, {'node': {'author': {'login': 'lesteve'}, 'bodyText': 'On my machine, setting tol=1e-5 (rather than the default 1e-4) makes the test pass. This does look like a behaviour change though, not sure whether this was intended or not ü§î ...', 'createdAt': '2025-01-21T10:19:12Z'}}, {'node': {'author': {'login': 'jeremiedbb'}, 'bodyText': 'I was looking into it, the issue is that we are comparing against the exact solution so we need a  good convergence. I opened #30688', 'createdAt': '2025-01-21T10:20:12Z'}}, {'node': {'author': {'login': 'scikit-learn-bot'}, 'bodyText': 'CI is no longer failing! ‚úÖ\\nSuccessful run on Jan 23, 2025', 'createdAt': '2025-01-22T03:02:09Z'}}]}}}, {'node': {'title': 'Possible bug in sklearn 1.6.1 PartialDependenceDisplay.from_estimator when target and feature are both binary', 'number': 30675, 'createdAt': '2025-01-20T00:00:08Z', 'url': 'https://github.com/scikit-learn/scikit-learn/issues/30675', 'bodyText': 'Describe the bug\\nPartialDependenceDisplay.from_estimator does not seem able to handle dummy variables when the response variable is binary. See example below. The example works fine in 1.5.2 but returns ValueError: cannot reshape array of size 1 into shape (2) in 1.6.1\\nSteps/Code to Reproduce\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.inspection import PartialDependenceDisplay\\n\\nnp.random.seed(42)\\nn_samples = 1000\\nage = np.random.normal(35, 10, n_samples)\\nsmoker = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\\nprob_disease = 1 / (1 + np.exp(-(age - 35) / 10 - 2 * smoker))\\nheart_disease = (np.random.random(n_samples) < prob_disease).astype(int)\\ndf = pd.DataFrame({\"age\": age, \"smoker\": smoker, \"heart_disease\": heart_disease})\\nX = df[[\"age\", \"smoker\"]]\\ny = df[\"heart_disease\"]\\n\\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\\nrf_model.fit(X, y)\\n\\npdp_age = PartialDependenceDisplay.from_estimator(rf_model, X, features=[0, 1])\\nExpected Results\\nPDP plots for age and smoker.\\nActual Results\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\nCell In[1], [line 19](vscode-notebook-cell:?execution_count=1&line=19)\\n     [16](vscode-notebook-cell:?execution_count=1&line=16) rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\\n     [17](vscode-notebook-cell:?execution_count=1&line=17) rf_model.fit(X, y)\\n---> [19](vscode-notebook-cell:?execution_count=1&line=19) pdp_age = PartialDependenceDisplay.from_estimator(rf_model, X, features=[0, 1])\\n\\nFile ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:707, in PartialDependenceDisplay.from_estimator(cls, estimator, X, features, sample_weight, categorical_features, feature_names, target, response_method, n_cols, grid_resolution, percentiles, method, n_jobs, verbose, line_kw, ice_lines_kw, pd_line_kw, contour_kw, ax, kind, centered, subsample, random_state)\\n    [701](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:701)         raise ValueError(\\n    [702](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:702)             f\"When a floating-point, subsample={subsample} should be in \"\\n    [703](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:703)             \"the (0, 1) range.\"\\n    [704](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:704)         )\\n    [706](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:706) # compute predictions and/or averaged predictions\\n--> [707](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:707) pd_results = Parallel(n_jobs=n_jobs, verbose=verbose)(\\n    [708](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:708)     delayed(partial_dependence)(\\n    [709](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:709)         estimator,\\n    [710](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:710)         X,\\n    [711](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:711)         fxs,\\n    [712](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:712)         sample_weight=sample_weight,\\n    [713](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:713)         feature_names=feature_names,\\n    [714](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:714)         categorical_features=categorical_features,\\n    [715](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:715)         response_method=response_method,\\n    [716](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:716)         method=method,\\n    [717](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:717)         grid_resolution=grid_resolution,\\n    [718](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:718)         percentiles=percentiles,\\n    [719](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:719)         kind=kind_plot,\\n    [720](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:720)     )\\n    [721](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:721)     for kind_plot, fxs in zip(kind_, features)\\n    [722](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:722) )\\n    [724](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:724) # For multioutput regression, we can only check the validity of target\\n    [725](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:725) # now that we have the predictions.\\n    [726](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:726) # Also note: as multiclass-multioutput classifiers are not supported,\\n    [727](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:727) # multiclass and multioutput scenario are mutually exclusive. So there is\\n    [728](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:728) # no risk of overwriting target_idx here.\\n    [729](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:729) pd_result = pd_results[0]  # checking the first result is enough\\n\\nFile ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:77, in Parallel.__call__(self, iterable)\\n     [72](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:72) config = get_config()\\n     [73](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:73) iterable_with_config = (\\n     [74](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:74)     (_with_config(delayed_func, config), args, kwargs)\\n     [75](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:75)     for delayed_func, args, kwargs in iterable\\n     [76](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:76) )\\n---> [77](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:77) return super().__call__(iterable_with_config)\\n\\nFile ~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1918, in Parallel.__call__(self, iterable)\\n   [1916](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1916)     output = self._get_sequential_output(iterable)\\n   [1917](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1917)     next(output)\\n-> [1918](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1918)     return output if self.return_generator else list(output)\\n   [1920](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1920) # Let\\'s create an ID that uniquely identifies the current call. If the\\n   [1921](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1921) # call is interrupted early and that the same instance is immediately\\n   [1922](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1922) # re-used, this id will be used to prevent workers that were\\n   [1923](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1923) # concurrently finalizing a task from the previous call to run the\\n   [1924](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1924) # callback.\\n   [1925](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1925) with self._lock:\\n\\nFile ~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1847, in Parallel._get_sequential_output(self, iterable)\\n   [1845](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1845) self.n_dispatched_batches += 1\\n   [1846](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1846) self.n_dispatched_tasks += 1\\n-> [1847](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1847) res = func(*args, **kwargs)\\n   [1848](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1848) self.n_completed_tasks += 1\\n   [1849](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1849) self.print_progress()\\n\\nFile ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:139, in _FuncWrapper.__call__(self, *args, **kwargs)\\n    [137](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:137)     config = {}\\n    [138](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:138) with config_context(**config):\\n--> [139](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:139)     return self.function(*args, **kwargs)\\n\\nFile ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216, in validate_params.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\\n    [210](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:210) try:\\n    [211](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:211)     with config_context(\\n    [212](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:212)         skip_parameter_validation=(\\n    [213](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213)             prefer_skip_nested_validation or global_skip_validation\\n    [214](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:214)         )\\n    [215](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:215)     ):\\n--> [216](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216)         return func(*args, **kwargs)\\n    [217](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:217) except InvalidParameterError as e:\\n    [218](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218)     # When the function is just a wrapper around an estimator, we allow\\n    [219](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:219)     # the function to delegate validation to the estimator, but we replace\\n    [220](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:220)     # the name of the estimator by the name of the function in the error\\n    [221](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:221)     # message to avoid confusion.\\n    [222](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:222)     msg = re.sub(\\n    [223](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:223)         r\"parameter of \\\\w+ must be\",\\n    [224](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:224)         f\"parameter of {func.__qualname__} must be\",\\n    [225](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:225)         str(e),\\n    [226](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:226)     )\\n\\nFile ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:682, in partial_dependence(estimator, X, features, sample_weight, categorical_features, feature_names, response_method, percentiles, grid_resolution, method, kind)\\n    [676](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:676)     averaged_predictions = _partial_dependence_recursion(\\n    [677](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:677)         estimator, grid, features_indices\\n    [678](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:678)     )\\n    [680](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:680) # reshape averaged_predictions to\\n    [681](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:681) # (n_outputs, n_values_feature_0, n_values_feature_1, ...)\\n--> [682](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:682) averaged_predictions = averaged_predictions.reshape(\\n    [683](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:683)     -1, *[val.shape[0] for val in values]\\n    [684](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:684) )\\n    [685](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:685) pdp_results = Bunch(grid_values=values)\\n    [687](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:687) if kind == \"average\":\\n\\nValueError: cannot reshape array of size 1 into shape (2)\\n\\nVersions\\nSystem:\\n    python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\\nexecutable: /Users/vnijs/miniconda/envs/msba/bin/python\\n   machine: macOS-14.2.1-arm64-arm-64bit\\n\\nPython dependencies:\\n      sklearn: 1.6.1\\n          pip: 24.3.1\\n   setuptools: 75.1.0\\n        numpy: 2.2.1\\n        scipy: 1.15.1\\n       Cython: None\\n       pandas: 2.2.3\\n   matplotlib: 3.10.0\\n       joblib: 1.4.2\\nthreadpoolctl: 3.5.0\\n\\nBuilt with OpenMP: True\\n\\nthreadpoolctl info:\\n       user_api: openmp\\n   internal_api: openmp\\n    num_threads: 16\\n         prefix: libomp\\n       filepath: /Users/vnijs/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib\\n        version: None', 'comments': {'edges': [{'node': {'author': {'login': 'lesteve'}, 'bodyText': 'Thanks for the issue, I confirm your snippet works with 1.5.2 and fails with 1.6.1, so this seems like a regression in 1.6.\\nThis looked somewhat similar to #30271 but is slightly different ...', 'createdAt': '2025-01-20T09:32:03Z'}}, {'node': {'author': {'login': 'lesteve'}, 'bodyText': 'Hmmm actually I misremembered and thought the PR fixing the issue was merged in time for 1.6.1 but it is only in main #26202.\\nRight now on main your snippet gives the following warning:\\n/home/lesteve/dev/scikit-learn/sklearn/inspection/_partial_dependence.py:709: FutureWarning: The column 1 contains integer data. Partial dependence plots are not supported for integer data: this can lead to implicit rounding with NumPy arrays or even errors with newer pandas versions. Please convert numerical featuresto floating point dtypes ahead of time to avoid problems. This will raise ValueError in scikit-learn 1.9.\\n  warnings.warn(\\n\\n@vnijs if you wish you can you try installing the development version following these instructions.', 'createdAt': '2025-01-20T13:58:12Z'}}, {'node': {'author': {'login': 'lesteve'}, 'bodyText': '@glemaitre when you have the time, is this worth thinking about a 1.6.2 release for this, or do you think this can wait for 1.7?', 'createdAt': '2025-01-23T13:18:36Z'}}]}}}]}}}}\n"
          ]
        }
      ],
      "source": [
        "results = fetch_repos_issues_by_name('scikit-learn', 'scikit-learn', token=None)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fZztYE6dpKP4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZztYE6dpKP4",
        "outputId": "fb2140ad-d2c5-4218-b75f-b9cffb248391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.59.3)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.14)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.29)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.4)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.12.14)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.13)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
            "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\n",
            "Successfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install openai langchain pinecone-client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "pGzbQx4044Ws",
      "metadata": {
        "id": "pGzbQx4044Ws"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone,ServerlessSpec\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=None,  # Replace with your Pinecone API key\n",
        ")\n",
        "\n",
        "# # Create or connect to an index\n",
        "\n",
        "# if index_name not in pinecone.list_indexes():\n",
        "#     pinecone.create_index(index_name, dimension=1536)  # Use 1536 for OpenAI embeddings\n",
        "# index = pinecone.Index(index_name)\n",
        "\n",
        "index_name = \"github-issues\"\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=1536, # Replace with your model dimensions\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\"\n",
        "    )\n",
        ")\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "wnyrIbsd7nwC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnyrIbsd7nwC",
        "outputId": "d498f7af-f7b6-4d21-b222-bbaca15eb0cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.11)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.15 (from langchain-community)\n",
            "  Downloading langchain-0.3.15-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.31 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.31-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.15->langchain-community) (0.3.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.15->langchain-community) (2.10.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.13)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.31->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.15->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.15->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.15-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.15-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.31-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.29\n",
            "    Uninstalling langchain-core-0.3.29:\n",
            "      Successfully uninstalled langchain-core-0.3.29\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.14\n",
            "    Uninstalling langchain-0.3.14:\n",
            "      Successfully uninstalled langchain-0.3.14\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.15 langchain-community-0.3.15 langchain-core-0.3.31 marshmallow-3.26.0 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3uh95IG--YFk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uh95IG--YFk",
        "outputId": "cb3cf153-82c5-49d9-81f5-7619f825ef14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "sWh569gn7GWL",
      "metadata": {
        "id": "sWh569gn7GWL"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from pinecone import Index\n",
        "\n",
        "def process_issues_and_store_hybrid(issues, index: Index):\n",
        "    \"\"\"\n",
        "    Process GitHub issues and comments, and store their embeddings in Pinecone using a hybrid approach.\n",
        "\n",
        "    Args:\n",
        "        repo_name (str): The name of the GitHub repository.\n",
        "        owner (str): The owner of the GitHub repository.\n",
        "        token (str): GitHub personal access token.\n",
        "        index (Index): Pinecone index instance to store embeddings.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the OpenAI Embeddings model\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\",openai_api_key=None)\n",
        "\n",
        "    for issue in issues[\"data\"][\"repository\"][\"issues\"][\"edges\"]:\n",
        "        issue_node = issue[\"node\"]\n",
        "        issue_title = issue_node[\"title\"]\n",
        "        issue_body = issue_node[\"bodyText\"]\n",
        "        issue_url = issue_node[\"url\"]\n",
        "        issue_id = issue_node[\"number\"]\n",
        "\n",
        "        # Embed issue (title + body)\n",
        "        issue_text = f\"Title: {issue_title}\\n\\nBody: {issue_body}\"\n",
        "        issue_vector = embeddings.embed_query(issue_text)\n",
        "\n",
        "        # Store issue embedding\n",
        "        issue_metadata = {\n",
        "            \"type\": \"issue\",\n",
        "            \"title\": issue_title,\n",
        "            \"url\": issue_url,\n",
        "            \"body\": issue_body,\n",
        "            \"createdAt\": issue_node[\"createdAt\"],\n",
        "            \"number\": issue_id\n",
        "        }\n",
        "        index.upsert([(str(issue_id), issue_vector, issue_metadata)])\n",
        "\n",
        "        # Process and embed comments\n",
        "        comments = issue_node[\"comments\"][\"edges\"]\n",
        "        comment_vectors = []\n",
        "        comment_texts = []\n",
        "\n",
        "        for comment in comments:\n",
        "            comment_node = comment[\"node\"]\n",
        "            comment_text = comment_node[\"bodyText\"]\n",
        "            comment_author = comment_node[\"author\"][\"login\"]\n",
        "            comment_created_at = comment_node[\"createdAt\"]\n",
        "\n",
        "            # Skip empty or invalid comments\n",
        "            if not comment_text or len(comment_text) < 10:\n",
        "                continue\n",
        "\n",
        "            # Embed comment\n",
        "            comment_vector = embeddings.embed_query(comment_text)\n",
        "\n",
        "            # Prepare comment metadata\n",
        "            comment_metadata = {\n",
        "                \"type\": \"comment\",\n",
        "                \"comment_text\": comment_text,\n",
        "                \"author\": comment_author,\n",
        "                \"createdAt\": comment_created_at,\n",
        "                \"issue_id\": issue_id,\n",
        "                \"issue_title\": issue_title\n",
        "            }\n",
        "            comment_vectors.append((f\"comment-{comment_created_at}\", comment_vector, comment_metadata))\n",
        "            comment_texts.append(f\"- {comment_author}: {comment_text}\")\n",
        "\n",
        "        # Store comment embeddings\n",
        "        if comment_vectors:\n",
        "            index.upsert(comment_vectors)\n",
        "\n",
        "        # Embed combined text (issue + comments)\n",
        "        combined_text = f\"Title: {issue_title}\\n\\nBody: {issue_body}\\n\\nComments:\\n\" + \"\\n\".join(comment_texts)\n",
        "        combined_vector = embeddings.embed_query(combined_text)\n",
        "\n",
        "        # Store combined embedding\n",
        "        combined_metadata = {\n",
        "            \"type\": \"combined\",\n",
        "            \"title\": issue_title,\n",
        "            \"url\": issue_url,\n",
        "            \"body\": issue_body,\n",
        "            \"createdAt\": issue_node[\"createdAt\"],\n",
        "            \"number\": issue_id\n",
        "        }\n",
        "        index.upsert([(f\"combined-{issue_id}\", combined_vector, combined_metadata)])\n",
        "\n",
        "    print(\"Issues, comments, and combined embeddings stored successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "hGphC4EI7yTS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGphC4EI7yTS",
        "outputId": "d71674cd-c439-4da6-ab4f-0eef3dfde8b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Issues, comments, and combined embeddings stored successfully!\n"
          ]
        }
      ],
      "source": [
        "process_issues_and_store_hybrid(results, index=index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "3WqKn2fHBQiY",
      "metadata": {
        "id": "3WqKn2fHBQiY"
      },
      "outputs": [],
      "source": [
        "def hybrid_search(query_text, index, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform a hybrid search across issues, comments, and combined embeddings.\n",
        "\n",
        "    Args:\n",
        "        query_text (str): The search query.\n",
        "        index (Index): Pinecone index instance to query.\n",
        "        top_k (int): Number of top results to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        dict: Results grouped by type (issues, comments, combined).\n",
        "    \"\"\"\n",
        "    # Embed query\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\",openai_api_key=None)\n",
        "    query_vector = embeddings.embed_query(query_text)\n",
        "\n",
        "    # Query the index for each type\n",
        "    results = {}\n",
        "    for type_filter in [\"issue\",\"comment\"]:\n",
        "        # results[type_filter] = index.query(\n",
        "        #     query_vector,\n",
        "        #     top_k=top_k,\n",
        "        #     include_metadata=True,\n",
        "        #     filter={\"type\": type_filter}\n",
        "        # )[\"matches\"]\n",
        "        results[type_filter] = index.query(\n",
        "            vector=query_vector,\n",
        "            top_k=1,\n",
        "            include_metadata=True,\n",
        "            filter={\"type\": type_filter}\n",
        "        )[\"matches\"]\n",
        "    # results = index.query(\n",
        "    # vector=query_vector,\n",
        "    # top_k=3,\n",
        "    # include_values=True,\n",
        "    # filter={\"type\": \"comment\"}\n",
        "    # )\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "sWXgDgZJBcPd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWXgDgZJBcPd",
        "outputId": "faf80c4a-39ba-482b-f2a3-1fccc9e7b84f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for issue:\n",
            " - Title: MNT Make binary display method parameters' order consistent\n",
            "   URL: https://github.com/scikit-learn/scikit-learn/issues/30717\n",
            "   Score: 0.757070601\n",
            "\n",
            "Results for comment:\n",
            " - Title: None\n",
            "   URL: None\n",
            "   Score: 0.767722905\n",
            "\n",
            "Results for combined:\n",
            " - Title: MNT Make binary display method parameters' order consistent\n",
            "   URL: https://github.com/scikit-learn/scikit-learn/issues/30717\n",
            "   Score: 0.736943364\n",
            "\n"
          ]
        }
      ],
      "source": [
        "outputs = hybrid_search(query_text=\"for this issue about Make binary display method parameters, have it resolved from the comment?\", index=index)\n",
        "# Display results\n",
        "for category, matches in outputs.items():\n",
        "    print(f\"Results for {category}:\")\n",
        "    for m in matches:\n",
        "        print(f\" - Title: {m['metadata'].get('title')}\")\n",
        "        print(f\"   URL: {m['metadata'].get('url')}\")\n",
        "        print(f\"   Score: {m['score']}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "PE1zRuXFfz_I",
      "metadata": {
        "id": "PE1zRuXFfz_I"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def generate_response_from_retrieval(query, retrieval_results):\n",
        "    \"\"\"\n",
        "    Generate a user-friendly response based on retrieval results.\n",
        "\n",
        "    Args:\n",
        "        query (str): User's query.\n",
        "        retrieval_results (dict): Grouped retrieval results by categories.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response.\n",
        "    \"\"\"\n",
        "    # Start building the context\n",
        "    context = f\"User Query: {query}\\n\\n\"\n",
        "\n",
        "    # Process each category in the retrieval results\n",
        "    for category, results in retrieval_results.items():\n",
        "        if results:  # Only include non-empty categories\n",
        "            context += f\"### {category.capitalize()}s:\\n\"\n",
        "            for idx, match in enumerate(results, start=1):\n",
        "                metadata = match.get(\"metadata\", {})\n",
        "                title = metadata.get(\"title\", \"N/A\")\n",
        "                body = metadata.get(\"body\", \"N/A\")\n",
        "                comment_text = metadata.get(\"comment_text\", \"No comments available\")\n",
        "                url = metadata.get(\"url\", \"No URL provided\")\n",
        "                created_at = metadata.get(\"createdAt\", \"Unknown date\")\n",
        "\n",
        "                # Add details for each result\n",
        "                context += f\"{idx}. Title: {title}\\n\"\n",
        "                context += f\"   Created At: {created_at}\\n\"\n",
        "                if category == \"comment\":\n",
        "                    context += f\"   Comment: {comment_text}\\n\"\n",
        "                else:\n",
        "                    context += f\"   Description: {body}\\n\"\n",
        "                context += f\"   URL: {url}\\n\\n\"\n",
        "\n",
        "    # Combine context into a prompt\n",
        "    prompt = f\"\"\"\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Based on the above information, provide a helpful response to the user's query.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate response using OpenAI GPT\n",
        "    # response = openai.ChatCompletion.create(\n",
        "    #     model=\"gpt-4\",\n",
        "    #     messages=[\n",
        "    #         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    #         {\"role\": \"user\", \"content\": prompt}\n",
        "    #     ]\n",
        "    # )\n",
        "    client = OpenAI(api_key=None)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ])\n",
        "\n",
        "    return completion.choices[0].message.content\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "ZAnQq_p8f7ew",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAnQq_p8f7ew",
        "outputId": "ebbe6400-255c-4732-c7b7-fd40c1dca5d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It appears that the issue regarding making scikit-learn's OpenML integration more generic for data download URLs has not yet been resolved based on the available comment. The comment suggests that the URLs in the dataset description should correctly point to the respective files (ARFF or Parquet), but doesn't confirm if there are any adjustments needed in scikit-learn's code or if it's purely an issue with OpenML's dataset links.\n",
            "\n",
            "For more information or to check the current status of the issue, you can visit the issue page on GitHub using the following link: [Scikit-learn Issue #30699](https://github.com/scikit-learn/scikit-learn/issues/30699). If you have specific concerns or need more assistance, it may be helpful to engage directly on that issue page or in the related discussion.\n"
          ]
        }
      ],
      "source": [
        "query = \"for this issue about OpenML, have it resolved from the comment? could you also provide me the issue link?\"\n",
        "retrieval_results=hybrid_search(query, index)\n",
        "grouped_results = generate_response_from_retrieval(query,retrieval_results)\n",
        "print(grouped_results)\n",
        "# response = generate_response_from_grouped_results(query, grouped_results)\n",
        "# print(response)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "dlin24799 (Jan 26, 2025, 12:02:05‚ÄØPM)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
